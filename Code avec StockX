# Installer les bibliothèques nécessaires (à exécuter une seule fois)
install.packages("blastula")
install.packages("keyring")
install.packages("rvest")
install.packages("dplyr")
library(blastula)
library(keyring)
library(rvest)
library(dplyr)

# Liste des URL de recherche
urls <- c(
  "https://www.ebay.fr/sch/i.html?_nkw=chaussures+de+course&_sop=12",
  "https://stockx.com/search/sneakers?s=running+shoes"
)

# Fonction pour extraire les données brutes et les filtrer
extract_filtered_data <- function(url) {
  page <- read_html(url)
  
  if (grepl("ebay", url)) {
    # Extraction pour eBay
    names <- page %>%
      html_nodes(".s-item__title") %>%
      html_text()
    
    prices <- page %>%
      html_nodes(".s-item__price") %>%
      html_text()
    
    links <- page %>%
      html_nodes(".s-item__link") %>%
      html_attr("href")
    
    valid_names <- names[!grepl("Shop on eBay", names)]
    valid_prices <- prices[!grepl("Shop on eBay", names)]
    valid_links <- links[!grepl("Shop on eBay", names)]
  } else if (grepl("stockx", url)) {
    # Extraction pour StockX
    names <- page %>%
      html_nodes(".css-3umptl") %>%
      html_text()
    
    prices <- page %>%
      html_nodes(".css-wk5cvz") %>%
      html_text()
    
    links <- page %>%
      html_nodes(".css-3umptl a") %>%
      html_attr("href")
    
    # Ajouter le domaine principal pour les liens relatifs
    links <- paste0("https://stockx.com", links)
    
    valid_names <- names
    valid_prices <- prices
    valid_links <- links
  } else {
    # Retourner un dataframe vide si le site n'est pas reconnu
    return(data.frame(Name = character(), Price = character(), Link = character()))
  }
  
  # Vérifier que toutes les longueurs correspondent
  min_length <- min(length(valid_names), length(valid_prices), length(valid_links))
  if (min_length == 0) {
    return(data.frame(Name = character(), Price = character(), Link = character()))
  }
  
  # Créer le data frame avec les données valides
  result <- data.frame(
    Name = valid_names[1:min_length],
    Price = valid_prices[1:min_length],
    Link = valid_links[1:min_length],
    stringsAsFactors = FALSE
  )
  
  return(result)
}

# Appliquer la fonction sur les URLs
filtered_data <- lapply(urls, extract_filtered_data)

# Combiner les résultats extraits
final_data <- do.call(rbind, filtered_data)

# Nettoyer les prix pour extraire la première valeur dans une plage de prix (si applicable)
final_data <- final_data %>%
  mutate(
    # Extraire les prix et les convertir en numérique (gérer les plages de prix)
    Price_num = gsub("[^0-9,]", "", Price),
    Price_num = gsub(",", ".", Price_num),
    Price_num = as.numeric(sapply(strsplit(Price_num, " à "), `[`, 1))
  ) %>%
  # Filtrer les lignes où Price_num est NA (valeurs non convertibles)
  filter(!is.na(Price_num)) %>%
  # Filtrer les prix entre 60 et 150 EUR
  filter(Price_num > 60 & Price_num < 150) %>%
  select(Name, Price, Price_num, Link)

# Afficher les résultats filtrés
print(final_data)

# Créer un message avec les liens des chaussures
email_body <- paste(
  "Projet Monsieur Pelletier: donner une deuxième vie aux chaussures de course à pied :\n\n",
  paste(final_data$Name, " - Prix: ", final_data$Price, " - Lien: ", final_data$Link, collapse = "\n"),
  sep = ""
)

# Créer l'email
email <- compose_email(
  body = md(email_body)
)

# Envoyer l'email avec les liens
smtp_send(
  email = email,
  from = "bricekubler@gmail.com",            # Remplacez par votre adresse Gmail
  to = "emilienhartmann@gmail.com",          # Adresse du destinataire
  subject = "Projet MPE1 DATA ANALYSIS: Chaussures de course trouvées",
  credentials = creds_key("gmail_correct")   # Utilisez la clé SMTP que vous avez créée
)
